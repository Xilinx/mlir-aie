{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ac7c13-4c9d-4e2e-a2c4-ed84a69a6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_scale after first conv1x1: 9.0\n",
      "Golden::Brevitas:: [[[[174  10   0 ...  15 217  50]\n",
      "   [ 78   0  47 ... 204  29 158]\n",
      "   [  0   0  43 ...  42   0  65]\n",
      "   ...\n",
      "   [  0   0   0 ...   0   0  66]\n",
      "   [  0   0 240 ... 158  33 248]\n",
      "   [ 69  48   0 ...   0   0   0]]\n",
      "\n",
      "  [[  0   0 130 ...   0  73  18]\n",
      "   [  0 179   2 ...  86 149  83]\n",
      "   [  0   1  54 ...   0  29   0]\n",
      "   ...\n",
      "   [  0 113  21 ...   0   0 162]\n",
      "   [ 39   0  43 ...  59   9   0]\n",
      "   [  0  24  91 ... 172  76 189]]\n",
      "\n",
      "  [[ 88 166   0 ...  83   0   0]\n",
      "   [ 45  64 126 ...   0  11   0]\n",
      "   [ 38   0 103 ...  25  50   0]\n",
      "   ...\n",
      "   [ 28  67   0 ...  59 136 255]\n",
      "   [ 25 139   0 ...   0   0   0]\n",
      "   [182 194   0 ...  71   0 173]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[  0  94 130 ...   0   0 174]\n",
      "   [  0   0  40 ...   3   0   0]\n",
      "   [108   0   0 ...  77  19 143]\n",
      "   ...\n",
      "   [ 80   0   0 ...  72   0   0]\n",
      "   [ 44  26   0 ...   0   0   0]\n",
      "   [133   0   0 ...  36   0 107]]\n",
      "\n",
      "  [[  0  27   0 ...   0  65   0]\n",
      "   [  0   0   0 ... 130  15 109]\n",
      "   [  0  72  70 ...   0   0  45]\n",
      "   ...\n",
      "   [ 59  12  87 ...  52   0   0]\n",
      "   [  0   0   0 ...  52   0   0]\n",
      "   [  0  26  72 ...  81  83  95]]\n",
      "\n",
      "  [[ 60 175   0 ...   0   0  73]\n",
      "   [255  29  46 ... 100   0  35]\n",
      "   [102   0   0 ...  94  12   9]\n",
      "   ...\n",
      "   [  0   1   7 ...  72   0 125]\n",
      "   [  0  55   0 ...  37   0  32]\n",
      "   [136   0   0 ...   0   0   0]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/core/TensorImpl.h:1791.)\n",
      "  return super().rename(names)\n",
      "C:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\brevitas\\export\\onnx\\standard\\manager.py:26: UserWarning: ONNX opset version set to 13, override with opset_version=\n",
      "  warnings.warn(f\"ONNX opset version set to {DEFAULT_OPSET}, override with {ka}=\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "total_wts (4096,)\n",
      "AIE output::: tensor([[[[174,  10,   0,  ...,  15, 217,  50],\n",
      "          [ 78,   0,  47,  ..., 204,  29, 158],\n",
      "          [  0,   0,  43,  ...,  42,   0,  65],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ...,   0,   0,  66],\n",
      "          [  0,   0, 240,  ..., 158,  33, 248],\n",
      "          [ 69,  48,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "         [[  0,   0, 130,  ...,   0,  73,  18],\n",
      "          [  0, 179,   2,  ...,  86, 149,  83],\n",
      "          [  0,   1,  54,  ...,   0,  29,   0],\n",
      "          ...,\n",
      "          [  0, 113,  21,  ...,   0,   0, 162],\n",
      "          [ 39,   0,  43,  ...,  59,   9,   0],\n",
      "          [  0,  24,  91,  ..., 172,  76, 189]],\n",
      "\n",
      "         [[ 88, 166,   0,  ...,  83,   0,   0],\n",
      "          [ 45,  64, 126,  ...,   0,  11,   0],\n",
      "          [ 38,   0, 103,  ...,  25,  50,   0],\n",
      "          ...,\n",
      "          [ 28,  67,   0,  ...,  59, 136, 255],\n",
      "          [ 25, 139,   0,  ...,   0,   0,   0],\n",
      "          [182, 194,   0,  ...,  71,   0, 173]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,  94, 130,  ...,   0,   0, 174],\n",
      "          [  0,   0,  40,  ...,   3,   0,   0],\n",
      "          [108,   0,   0,  ...,  77,  19, 143],\n",
      "          ...,\n",
      "          [ 80,   0,   0,  ...,  72,   0,   0],\n",
      "          [ 44,  26,   0,  ...,   0,   0,   0],\n",
      "          [133,   0,   0,  ...,  36,   0, 107]],\n",
      "\n",
      "         [[  0,  27,   0,  ...,   0,  65,   0],\n",
      "          [  0,   0,   0,  ..., 130,  15, 109],\n",
      "          [  0,  72,  70,  ...,   0,   0,  45],\n",
      "          ...,\n",
      "          [ 59,  12,  87,  ...,  52,   0,   0],\n",
      "          [  0,   0,   0,  ...,  52,   0,   0],\n",
      "          [  0,  26,  72,  ...,  81,  83,  95]],\n",
      "\n",
      "         [[ 60, 175,   0,  ...,   0,   0,  73],\n",
      "          [255,  29,  46,  ..., 100,   0,  35],\n",
      "          [102,   0,   0,  ...,  94,  12,   9],\n",
      "          ...,\n",
      "          [  0,   1,   7,  ...,  72,   0, 125],\n",
      "          [  0,  55,   0,  ...,  37,   0,  32],\n",
      "          [136,   0,   0,  ...,   0,   0,   0]]]], dtype=torch.uint8)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'brevitas.quant_tensor.QuantTensor'>\n",
      "difference:: tensor(0.0039, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Temp\\ipykernel_21516\\3926934365.py:203: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:367.)\n",
      "  print(\"difference::\",torch.max(torch.abs(ofm_mem_fmt*block_0_relu_1 - q_bottleneck_out)))\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../misc\")\n",
    "from utils import DataShaper\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import (\n",
    "    Int8ActPerTensorFixedPoint,\n",
    "    Int8WeightPerTensorFixedPoint,\n",
    "    Uint8ActPerTensorFixedPoint,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "design = \"conv1x1_cifar_scalar\"\n",
    "# design=\"conv1x1_cifar_vector\"\n",
    "# aie_teardown()\n",
    "sys.path.append(\"../../../utils\")\n",
    "import xrtutils\n",
    "\n",
    "xclbin_path = os.path.abspath(\"../baseline/\" + design + \"/build/final.xclbin\")\n",
    "insts_path = os.path.abspath(\"../baseline/\" + design + \"/build/insts.txt\")\n",
    "\n",
    "log_folder = \"log/log_\" + design\n",
    "\n",
    "enable_aie = True\n",
    "aie_is_setup = False\n",
    "enable_trace = True\n",
    "trace_file = \"traces/\" + design + \".txt\"\n",
    "\n",
    "app = None\n",
    "in_buf = None\n",
    "arg1_buf = None\n",
    "out_buf = None\n",
    "\n",
    "dtype_in = np.dtype(\"int8\")\n",
    "dtype_wts = np.dtype(\"int8\")\n",
    "dtype_out = np.dtype(\"uint8\")\n",
    "\n",
    "# shape_in_act = (256, 32,32)\n",
    "# shape_in_wts1 = (256,64,1,1) #output, input, kx,ky\n",
    "# shape_in_wts2 = (64,64,3,3) #output, input, kx,ky\n",
    "# shape_in_wts3 = (64,256,1,1) #output, input, kx,ky\n",
    "# shape_total_wts=(69632,1)\n",
    "# shape_out  = (256, 32,32)\n",
    "\n",
    "shape_in_act = (32, 8, 32, 8)  #'YCXC8' , 'CYX'\n",
    "shape_in_wts1 = (8, 8, 1, 1, 8, 8)  # out,in,ky,kx,in8,out8\n",
    "\n",
    "shape_total_wts = (4096, 1)\n",
    "shape_out = (32, 8, 32, 8)\n",
    "\n",
    "trace_size = 16384\n",
    "\n",
    "\n",
    "def setup_aie(\n",
    "    xclbin_path,\n",
    "    insts_path,\n",
    "    in_0_shape,\n",
    "    in_0_dtype,\n",
    "    in_1_shape,\n",
    "    in_1_dtype,\n",
    "    out_buf_shape,\n",
    "    out_buf_dtype,\n",
    "    enable_trace=False,\n",
    "    kernel_name=\"MLIR_AIE\",\n",
    "):\n",
    "    app = xrtutils.AIE_Application(xclbin_path, insts_path, kernel_name)\n",
    "    app.register_buffer(2, shape=in_0_shape, dtype=in_0_dtype)\n",
    "    app.register_buffer(3, shape=in_1_shape, dtype=in_1_dtype)\n",
    "    if enable_trace:\n",
    "        out_buf_len_bytes = np.prod(out_buf_shape) * np.dtype(out_buf_dtype).itemsize\n",
    "        out_buf_shape = (out_buf_len_bytes + trace_size,)\n",
    "        out_buf_dtype = np.uint8\n",
    "    app.register_buffer(4, shape=out_buf_shape, dtype=out_buf_dtype)\n",
    "    return app\n",
    "\n",
    "\n",
    "def extract_trace(out_buf, out_buf_shape, out_buf_dtype):\n",
    "    trace_size_words = trace_size // 4\n",
    "    out_buf_flat = out_buf.reshape((-1,)).view(np.uint32)\n",
    "    output_prefix = (\n",
    "        out_buf_flat[:-trace_size_words].view(out_buf_dtype).reshape(out_buf_shape)\n",
    "    )\n",
    "    trace_suffix = out_buf_flat[-trace_size_words:]\n",
    "    return output_prefix, trace_suffix\n",
    "\n",
    "\n",
    "def write_out_trace(trace, file_name):\n",
    "    out_str = \"\\n\".join(f\"{i:0{8}x}\" for i in trace if i != 0)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(out_str)\n",
    "\n",
    "\n",
    "app = setup_aie(\n",
    "    xclbin_path,\n",
    "    insts_path,\n",
    "    shape_in_act,\n",
    "    dtype_in,\n",
    "    shape_total_wts,\n",
    "    dtype_wts,\n",
    "    shape_out,\n",
    "    dtype_out,\n",
    "    enable_trace,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import (\n",
    "    Int8ActPerTensorFixedPoint,\n",
    "    Int8WeightPerTensorFixedPoint,\n",
    "    Uint8ActPerTensorFixedPoint,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "\n",
    "# input=torch.ones(64,32,32)\n",
    "input = torch.randn(1, 64, 32, 32)\n",
    "# image_name = f'./cifar_images/image_0.png'\n",
    "# img = Image.open(image_name)\n",
    "# input_tensor = cifar_test_transform(img)\n",
    "# input = input_tensor.unsqueeze(0)\n",
    "# Use a separate QuantIdentity so that the quantized output can be fed to both layers\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "ds = DataShaper()\n",
    "\n",
    "\n",
    "def init_pad_input(x, input_channels, desired_channels=4):\n",
    "    padding = torch.zeros(1, input_channels * (desired_channels - 1), 32, 32)\n",
    "    return torch.cat((x, padding), 1)\n",
    "\n",
    "\n",
    "# try:\n",
    "for i in range(0, 1):\n",
    "\n",
    "    class QuantBottleneck_projected(nn.Module):\n",
    "        expansion = 4\n",
    "\n",
    "        def __init__(self, in_planes=64, planes=64):\n",
    "            super(QuantBottleneck_projected, self).__init__()\n",
    "            self.quant_id_1 = QuantIdentity(\n",
    "                act_quant=Int8ActPerTensorFixedPoint,\n",
    "                bit_width=8,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "            self.quant_conv1 = QuantConv2d(\n",
    "                in_planes,\n",
    "                planes,\n",
    "                kernel_size=1,\n",
    "                bit_width=8,\n",
    "                weight_bit_width=8,\n",
    "                bias=False,\n",
    "                weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "            self.quant_relu1 = QuantReLU(\n",
    "                act_quant=Uint8ActPerTensorFixedPoint,\n",
    "                bit_width=8,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            out_q = self.quant_id_1(x)\n",
    "            out = self.quant_conv1(out_q)\n",
    "            out = self.quant_relu1(out)\n",
    "            return out\n",
    "\n",
    "    quant_bottleneck_model = QuantBottleneck_projected()\n",
    "\n",
    "    quant_id_1 = QuantIdentity(\n",
    "        act_quant=Int8ActPerTensorFixedPoint, bit_width=8, return_quant_tensor=True\n",
    "    )\n",
    "    quant_bottleneck_model.eval()\n",
    "    quant_id_1.eval()\n",
    "\n",
    "    init_scale = quant_bottleneck_model.quant_id_1.quant_act_scale()\n",
    "    block_0_relu_1 = quant_bottleneck_model.quant_relu1.quant_act_scale()\n",
    "\n",
    "    block_0_weight_scale1 = quant_bottleneck_model.quant_conv1.quant_weight_scale()\n",
    "\n",
    "    block_0_combined_scale1 = -torch.log2(\n",
    "        init_scale * block_0_weight_scale1 / block_0_relu_1\n",
    "    )\n",
    "\n",
    "    print(\"combined_scale after first conv1x1:\", block_0_combined_scale1.item())\n",
    "\n",
    "    block_0_int_weight_1 = quant_bottleneck_model.quant_conv1.quant_weight().int(\n",
    "        float_datatype=True\n",
    "    )\n",
    "\n",
    "    q_bottleneck_out = quant_bottleneck_model(input)\n",
    "    gold_out = q_bottleneck_out.int(float_datatype=True).data.numpy().astype(dtype_out)\n",
    "    print(\"Golden::Brevitas::\", gold_out)\n",
    "    gold_out.tofile(log_folder + \"/gold_out.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    from brevitas.export import export_onnx_qcdq\n",
    "\n",
    "    # ref_input = torch.ones(1, 3, 32, 32, device=\"cpu\", dtype=dtype)\n",
    "    export_onnx_qcdq(quant_bottleneck_model, input, log_folder + \"/\" + design + \".onnx\")\n",
    "    # # Brevitas convolution\n",
    "    q_inp = quant_id_1(input)\n",
    "    int_inp = q_inp.int(float_datatype=True)\n",
    "\n",
    "    before_input = int_inp.squeeze().data.numpy().astype(dtype_in)\n",
    "    # print(before_input)\n",
    "    before_input.tofile(\n",
    "        log_folder + \"/before_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\"\n",
    "    )\n",
    "    # ifm_mem_fmt = ds.reorder_mat(int_inp.squeeze().data.numpy().astype(dtype_in),'YCX' , 'CYX' )\n",
    "    # ifm_mem_fmt = ds.reorder_mat(before_input,'YCX' , 'CYX' )\n",
    "    ifm_mem_fmt = ds.reorder_mat(before_input, \"YCXC8\", \"CYX\")\n",
    "    # print(\"Input after:::\",ifm_mem_fmt.reshape((32,64, 32)))\n",
    "    ifm_mem_fmt.tofile(log_folder + \"/after_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\")\n",
    "    # print(\"ifm_mem_fmt shape\",ifm_mem_fmt.shape)\n",
    "    # wts1 = ds.reorder_mat(int_weight1.data.numpy().astype(dtype_in),'OIYX' , 'OIYX' )\n",
    "    # wts2 = ds.reorder_mat(int_weight2.data.numpy().astype(dtype_in),'OIYX' , 'OIYX' )\n",
    "    # wts3 = ds.reorder_mat(int_weight3.data.numpy().astype(dtype_in),'OIYX' , 'OIYX' )\n",
    "    wts1 = ds.reorder_mat(\n",
    "        block_0_int_weight_1.data.numpy().astype(dtype_wts), \"OIYXI8O8\", \"OIYX\"\n",
    "    )\n",
    "\n",
    "    total_wts = np.concatenate((wts1), axis=None)\n",
    "    total_wts.tofile(log_folder + \"/weights_mem_fmt_final.txt\", sep=\",\", format=\"%d\")\n",
    "    print(\"total_wts\", total_wts.shape)\n",
    "    for i in range(0, 2):\n",
    "        app.buffers[2].write(ifm_mem_fmt)  # input's standard format CYX | scalar YCX\n",
    "        app.buffers[3].write(total_wts)  # wts's standard format OIYX | scalar OIYX\n",
    "        # app.buffers[3].write(int_weight2.data.numpy().astype(dtype_in),offset=2048) # wts's standard format OIYX | scalar OIYX\n",
    "        app.run()\n",
    "        output3 = app.buffers[4].read()\n",
    "        if enable_trace:\n",
    "            output3, trace = extract_trace(output3, shape_out, dtype_out)\n",
    "            write_out_trace(trace, trace_file)\n",
    "    # temp_out=output3.reshape(32,256, 32)\n",
    "    # ofm_mem_fmt = temp_out.swapaxes(0,1)\n",
    "    temp_out = output3.reshape(32, 8, 32, 8)\n",
    "    # print(\"AIE temp_out:::\",temp_out)\n",
    "\n",
    "    temp2_out = ds.reorder_mat(temp_out, \"CDYX\", \"YCXD\")\n",
    "    # print(\"AIE reorder temp_out:::\",temp_out)\n",
    "    ofm_mem_fmt = temp2_out.reshape(64, 32, 32)\n",
    "    ofm_mem_fmt.tofile(\n",
    "        log_folder + \"/after_ofm_mem_fmt_final.txt\", sep=\",\", format=\"%d\"\n",
    "    )\n",
    "\n",
    "    ofm_mem_fmt = torch.from_numpy(ofm_mem_fmt).unsqueeze(0)\n",
    "    print(\"AIE output:::\", ofm_mem_fmt)\n",
    "    print(type(ofm_mem_fmt))\n",
    "    print(type(q_bottleneck_out))\n",
    "    print(\n",
    "        \"difference::\",\n",
    "        torch.max(torch.abs(ofm_mem_fmt * block_0_relu_1 - q_bottleneck_out)),\n",
    "    )\n",
    "    assert np.allclose(ofm_mem_fmt, gold_out, rtol=0, atol=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c13fc6-5652-4ea8-bf19-b14cd5d1da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "if enable_trace:\n",
    "    print(trace)\n",
    "else:\n",
    "    print(\"tracing not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af167f5e-53f6-41e9-bd0b-5b3acbb052ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf1f0f-8b6f-4969-a19e-dfe0643d53bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
