{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ac7c13-4c9d-4e2e-a2c4-ed84a69a6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________POST PTQ SCALES_________\n",
      "init_scale: tensor(0.0078)\n",
      "block_0_relu1: tensor(0.0039)\n",
      "block_0_relu2: tensor(0.0039)\n",
      "block_0_relu3: tensor(0.0039)\n",
      "block_0_weight_scale1: tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "block_0_weight_scale2: tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "block_0_weight_scale3: tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "block_0_weight_scale_skip: tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "init_scale: tensor(0.0078)\n",
      "block_1_relu1: tensor(0.0039)\n",
      "block_1_relu2: tensor(0.0039)\n",
      "block_1_relu3: tensor(0.0039)\n",
      "--------------------------------------------------------------\n",
      "block_1_weight_scale1: tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "block_1_weight_scale2: tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "block_1_weight_scale3: tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "--------------------------------------------------------------\n",
      "combined_scale block0 after first conv1x1: 9.0\n",
      "combined_scale block0 after second conv3x3: 11.0\n",
      "combined_scale block0 after third conv1x1: 11.0\n",
      "combined_scale block0 after adding skip connection: -1.0\n",
      "combined_scale block0 after skip conv1x1: 10.0\n",
      "--------------------------------------------------------------\n",
      "combined_scale block1 after first conv1x1: 11.0\n",
      "combined_scale block1 after second conv3x3: 11.0\n",
      "combined_scale block1 after third conv1x1: 10.0\n",
      "combined_scale block1 after adding skip connection: -0.0\n",
      "Golden::Brevitas:: [[[[  5   3 139 ...   7   5   0]\n",
      "   [  9   8   0 ...   0   0   0]\n",
      "   [251   0   0 ...   0 113  69]\n",
      "   ...\n",
      "   [  7  63   0 ...   0   0 119]\n",
      "   [  0  28  90 ... 202  72 116]\n",
      "   [  2 103  20 ... 111   2   0]]\n",
      "\n",
      "  [[  2   3 171 ...  78   0  27]\n",
      "   [ 65 110   0 ...  65   0   0]\n",
      "   [121   0   1 ...   0   0   2]\n",
      "   ...\n",
      "   [ 77  26  32 ...   0  69 190]\n",
      "   [  0   0  42 ...   0   0   0]\n",
      "   [  0  36  45 ...   0   0  50]]\n",
      "\n",
      "  [[  0 130   0 ...   0 220 115]\n",
      "   [ 27   0   0 ...   9 124  59]\n",
      "   [  4   5 198 ...  31 247  20]\n",
      "   ...\n",
      "   [  0   0   0 ...  21   0  14]\n",
      "   [ 36   0   2 ...   0   1   3]\n",
      "   [ 96   4 114 ...   5  41   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 26  49  10 ...  83   9  16]\n",
      "   [ 64  12  21 ... 215  12   0]\n",
      "   [240  36  66 ...  13  20  12]\n",
      "   ...\n",
      "   [ 16  16  59 ... 124  43  43]\n",
      "   [  9  12  36 ...   4   6  20]\n",
      "   [ 60  48   0 ... 117   5   7]]\n",
      "\n",
      "  [[ 12   2   0 ... 158   3  40]\n",
      "   [ 54   2 143 ...   3  16   0]\n",
      "   [  4   1  10 ...  89  66   4]\n",
      "   ...\n",
      "   [ 86  99  60 ... 150  84   0]\n",
      "   [  1   4   5 ...   9  13   0]\n",
      "   [ 12  10   6 ...   3 105  80]]\n",
      "\n",
      "  [[ 73  73  74 ...  13   6   1]\n",
      "   [ 88 126  21 ...   0  59 174]\n",
      "   [  0 136   0 ... 122   3 136]\n",
      "   ...\n",
      "   [  0   0 167 ...   1   0  22]\n",
      "   [ 87   0 161 ...   0  88   1]\n",
      "   [  0  49   7 ...   0 104 110]]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/core/TensorImpl.h:1791.)\n",
      "  return super().rename(names)\n",
      "C:\\Users\\gagan\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\brevitas\\export\\onnx\\standard\\manager.py:26: UserWarning: ONNX opset version set to 13, override with opset_version=\n",
      "  warnings.warn(f\"ONNX opset version set to {DEFAULT_OPSET}, override with {ka}=\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Diagnostic Run torch.onnx.export version 2.0.1+cpu ==============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n",
      "total_wts (143360,)\n",
      "AIE output::: tensor([[[[  5,   3, 139,  ...,   7,   5,   0],\n",
      "          [  9,   8,   0,  ...,   0,   0,   0],\n",
      "          [251,   0,   0,  ...,   0, 113,  69],\n",
      "          ...,\n",
      "          [  7,  63,   0,  ...,   0,   0, 119],\n",
      "          [  0,  28,  90,  ..., 202,  72, 116],\n",
      "          [  2, 103,  20,  ..., 111,   2,   0]],\n",
      "\n",
      "         [[  2,   3, 171,  ...,  78,   0,  27],\n",
      "          [ 66, 110,   0,  ...,  65,   0,   0],\n",
      "          [121,   0,   1,  ...,   0,   0,   2],\n",
      "          ...,\n",
      "          [ 77,  26,  31,  ...,   0,  69, 190],\n",
      "          [  0,   0,  43,  ...,   0,   1,   0],\n",
      "          [  0,  36,  45,  ...,   0,   0,  50]],\n",
      "\n",
      "         [[  0, 130,   0,  ...,   0, 220, 115],\n",
      "          [ 27,   0,   0,  ...,   9, 124,  59],\n",
      "          [  4,   5, 198,  ...,  31, 247,  20],\n",
      "          ...,\n",
      "          [  0,   0,   0,  ...,  21,   0,  14],\n",
      "          [ 36,   0,   2,  ...,   0,   1,   3],\n",
      "          [ 96,   4, 114,  ...,   5,  41,   0]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 26,  49,  10,  ...,  83,   9,  16],\n",
      "          [ 64,  11,  21,  ..., 215,  12,   0],\n",
      "          [240,  36,  66,  ...,  13,  20,  12],\n",
      "          ...,\n",
      "          [ 16,  16,  59,  ..., 124,  43,  43],\n",
      "          [  9,  12,  36,  ...,   4,   5,  20],\n",
      "          [ 60,  48,   0,  ..., 117,   5,   7]],\n",
      "\n",
      "         [[ 12,   2,   0,  ..., 158,   3,  40],\n",
      "          [ 54,   2, 142,  ...,   3,  16,   0],\n",
      "          [  4,   1,  10,  ...,  90,  66,   4],\n",
      "          ...,\n",
      "          [ 86,  99,  60,  ..., 150,  84,   0],\n",
      "          [  1,   4,   5,  ...,   9,  13,   0],\n",
      "          [ 12,  10,   6,  ...,   3, 105,  80]],\n",
      "\n",
      "         [[ 73,  73,  74,  ...,  13,   6,   1],\n",
      "          [ 88, 126,  21,  ...,   0,  59, 174],\n",
      "          [  0, 136,   0,  ..., 122,   3, 136],\n",
      "          ...,\n",
      "          [  0,   0, 167,  ...,   1,   0,  22],\n",
      "          [ 87,   0, 161,  ...,   0,  88,   1],\n",
      "          [  0,  49,   7,  ...,   0, 104, 110]]]], dtype=torch.uint8)\n",
      "<class 'torch.Tensor'>\n",
      "<class 'brevitas.quant_tensor.QuantTensor'>\n",
      "difference:: tensor(255, dtype=torch.uint8)\n",
      "difference:: tensor(0.0117, grad_fn=<MaxBackward1>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gagan\\AppData\\Local\\Temp\\ipykernel_9248\\271668325.py:285: UserWarning: Defining your `__torch_function__` as a plain method is deprecated and will be an error in future, please define it as a classmethod. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:367.)\n",
      "  print(\"difference::\",torch.max(torch.abs(ofm_mem_fmt*block_1_relu_3 - q_bottleneck_out)))\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 286\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifference::\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(ofm_mem_fmt \u001b[38;5;241m-\u001b[39m gold_out)))\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifference::\u001b[39m\u001b[38;5;124m\"\u001b[39m,torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(ofm_mem_fmt\u001b[38;5;241m*\u001b[39mblock_1_relu_3 \u001b[38;5;241m-\u001b[39m q_bottleneck_out)))\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(np\u001b[38;5;241m.\u001b[39mallclose(ofm_mem_fmt, gold_out, rtol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m))\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"../misc\");\n",
    "from utils import DataShaper\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import Int8ActPerTensorFixedPoint, Int8WeightPerTensorFixedPoint,Uint8ActPerTensorFixedPoint\n",
    "\n",
    "torch.manual_seed(0)\n",
    "design=\"layer_1\"\n",
    "# aie_teardown()\n",
    "sys.path.append(\"../../../utils\"); import xrtutils\n",
    "xclbin_path = os.path.abspath(\"../network/\"+design+\"/build/final.xclbin\")\n",
    "insts_path  = os.path.abspath(\"../network/\"+design+\"/build/insts.txt\")\n",
    "\n",
    "log_folder=\"log/log_\"+design\n",
    "\n",
    "enable_aie = True\n",
    "aie_is_setup = False\n",
    "enable_trace = False\n",
    "trace_file='traces/trace.txt'\n",
    "\n",
    "app = None\n",
    "in_buf = None\n",
    "arg1_buf = None\n",
    "out_buf = None\n",
    "dtype_in  = np.dtype(\"int8\")\n",
    "dtype_wts  = np.dtype(\"int8\")\n",
    "dtype_out = np.dtype(\"uint8\")\n",
    "\n",
    "shape_in_act   = (32,8,32,8)  #'YCXC8' , 'CYX'\n",
    "# shape_in_wts1  = (8,8,1,1,8,8) #out,in,ky,kx,in8,out8\n",
    "# shape_in_wts2  = (8,8,3,3,8,8)  #out,in,ky,kx,in8,out8\n",
    "# shape_in_wts3  = (32,8,1,1,8,8)   #out,in,ky,kx,in8,out8\n",
    "shape_in_wts_skip  = (8,32,1,1,8,8) #out,in,ky,kx,in8,out8\n",
    "shape_total_wts= (143360,1)\n",
    "shape_out      = (32,32,32,8)\n",
    "\n",
    "trace_size = 8192\n",
    "\n",
    "def setup_aie(xclbin_path, insts_path, \n",
    "              in_0_shape, in_0_dtype,\n",
    "              in_1_shape, in_1_dtype, \n",
    "              out_buf_shape, out_buf_dtype,\n",
    "              enable_trace=False,\n",
    "              kernel_name=\"MLIR_AIE\"):\n",
    "    app = xrtutils.AIE_Application(xclbin_path, insts_path, kernel_name)\n",
    "    app.register_buffer(2, shape=in_0_shape, dtype=in_0_dtype)\n",
    "    app.register_buffer(3, shape=in_1_shape, dtype=in_1_dtype)\n",
    "    if enable_trace:\n",
    "      out_buf_len_bytes = np.prod(out_buf_shape) * np.dtype(out_buf_dtype).itemsize\n",
    "      out_buf_shape = (out_buf_len_bytes + trace_size, )\n",
    "      out_buf_dtype = np.uint8\n",
    "    app.register_buffer(4, shape=out_buf_shape, dtype=out_buf_dtype)\n",
    "    return app\n",
    "\n",
    "def extract_trace(out_buf, out_buf_shape, out_buf_dtype):\n",
    "    trace_size_words = trace_size//4\n",
    "    out_buf_flat = out_buf.reshape((-1,)).view(np.uint32)\n",
    "    output_prefix = out_buf_flat[:-trace_size_words].view(out_buf_dtype).reshape(out_buf_shape)\n",
    "    trace_suffix = out_buf_flat[-trace_size_words:]\n",
    "    return output_prefix, trace_suffix\n",
    "\n",
    "def write_out_trace(trace, file_name):\n",
    "    out_str = \"\\n\".join(f\"{i:0{8}x}\" \n",
    "                        for i in trace\n",
    "                        if i != 0)\n",
    "    with open(file_name, 'w') as f:\n",
    "      f.write(out_str)\n",
    "\n",
    "app = setup_aie(xclbin_path, insts_path,\n",
    "                            shape_in_act, dtype_in,      \n",
    "                            shape_total_wts,dtype_wts,\n",
    "                            shape_out, dtype_out,enable_trace)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import Int8ActPerTensorFixedPoint, Int8WeightPerTensorFixedPoint,Uint8ActPerTensorFixedPoint\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "input=torch.randn(1, 64,32,32)\n",
    "num_classes=10\n",
    "ds = DataShaper()\n",
    "def init_pad_input(x,input_channels,desired_channels=4):\n",
    "           padding=torch.zeros(1,input_channels*(desired_channels-1),32,32)\n",
    "           return torch.cat((x,padding),1)\n",
    "# try:\n",
    "for i in range (0,1): \n",
    "    class QuantBottleneck_projected(nn.Module):\n",
    "        expansion = 4\n",
    "        def __init__(self, in_planes=64, planes=64):\n",
    "            super(QuantBottleneck_projected, self).__init__()\n",
    "            # block 0\n",
    "            self.quant_id_1 = QuantIdentity(act_quant=Int8ActPerTensorFixedPoint,bit_width=8, return_quant_tensor=True) \n",
    "            self.quant_block0_conv1 = QuantConv2d(in_planes, planes, kernel_size=1,bit_width=8,weight_bit_width=8, bias=False, weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block0_conv2 = QuantConv2d(planes, planes, kernel_size=3,bit_width=8,weight_bit_width=8, bias=False,padding=1,padding_mode ='zeros', weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block0_conv3 = QuantConv2d(planes, self.expansion *planes, kernel_size=1,bit_width=8,weight_bit_width=8, bias=False,weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block0_relu1 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            self.quant_block0_relu2 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            self.quant_block0_relu3 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            self.quant_add_1 = QuantIdentity(act_quant=Int8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            # Quant_add_1 shares the scale factors with block0_relu3, however one is signed and the other one is unsigned\n",
    "            self.quant_add_1.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl = self.quant_block0_relu3.act_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl\n",
    "            self.quant_add_1.act_quant.fused_activation_quant_proxy.tensor_quant.int_scaling_impl = self.quant_block0_relu3.act_quant.fused_activation_quant_proxy.tensor_quant.int_scaling_impl\n",
    "            self.shortcut = QuantConv2d(in_planes, self.expansion*planes, kernel_size=1,bit_width=8,weight_bit_width=8, bias=False, weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            # block 1\n",
    "            # self.quant_id_2 = QuantIdentity(act_quant=Int8ActPerTensorFixedPoint,bit_width=8, return_quant_tensor=True) \n",
    "    \n",
    "            self.quant_block1_conv1 = QuantConv2d(self.expansion *in_planes, planes, kernel_size=1,bit_width=8,weight_bit_width=8, bias=False, weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block1_conv2 = QuantConv2d(planes, planes, kernel_size=3,bit_width=8,weight_bit_width=8, bias=False,padding=1,padding_mode ='zeros', weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block1_conv3 = QuantConv2d(planes, self.expansion *planes, kernel_size=1,bit_width=8,weight_bit_width=8, bias=False,weight_quant=Int8WeightPerTensorFixedPoint, return_quant_tensor=True)\n",
    "            self.quant_block1_relu1 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            self.quant_block1_relu2 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "            self.quant_block1_relu3 = QuantReLU(act_quant=Uint8ActPerTensorFixedPoint ,bit_width=8, return_quant_tensor=True)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            out_q = self.quant_id_1(x)\n",
    "            out_rhs = self.quant_block0_conv1(out_q)\n",
    "            out_rhs = self.quant_block0_relu1(out_rhs)\n",
    "            out_rhs = self.quant_block0_conv2(out_rhs)\n",
    "            out_rhs = self.quant_block0_relu2(out_rhs)\n",
    "            out_rhs = self.quant_block0_conv3(out_rhs)\n",
    "            out_rhs = self.quant_id_1(out_rhs)\n",
    "            out_lhs=self.shortcut(out_q)\n",
    "            out_lhs = self.quant_id_1(out_lhs)\n",
    "            out_block0 = out_rhs+out_lhs\n",
    "            out_block0 = self.quant_block0_relu3(out_block0)    \n",
    "            # block 1\n",
    "            out_rhs1 = self.quant_block1_conv1(out_block0)\n",
    "            out_rhs1 = self.quant_block1_relu1(out_rhs1)\n",
    "            out_rhs1 = self.quant_block1_conv2(out_rhs1)\n",
    "            out_rhs1 = self.quant_block1_relu2(out_rhs1)\n",
    "            out_rhs1 = self.quant_block1_conv3(out_rhs1)\n",
    "            out_rhs1 = self.quant_add_1(out_rhs1)\n",
    "            out_block1=out_block0+out_rhs1\n",
    "            out_block1 = self.quant_block1_relu3(out_block1)\n",
    "            \n",
    "            return out_block1\n",
    "                \n",
    "\n",
    "    \n",
    "    quant_bottleneck_model=QuantBottleneck_projected()\n",
    "    \n",
    "    quant_id_1 = QuantIdentity(act_quant=Int8ActPerTensorFixedPoint,bit_width=8, return_quant_tensor=True) \n",
    "    quant_bottleneck_model.eval()\n",
    "    quant_id_1.eval()\n",
    "  \n",
    "    init_scale = quant_bottleneck_model.quant_id_1.quant_act_scale()\n",
    "    block_0_relu_1 = quant_bottleneck_model.quant_block0_relu1.quant_act_scale()\n",
    "    block_0_relu_2= quant_bottleneck_model.quant_block0_relu2.quant_act_scale()\n",
    "    block_0_relu_3= quant_bottleneck_model.quant_block0_relu3.quant_act_scale()\n",
    "    \n",
    "    block_0_weight_scale1 = quant_bottleneck_model.quant_block0_conv1.quant_weight_scale()\n",
    "    block_0_weight_scale2 = quant_bottleneck_model.quant_block0_conv2.quant_weight_scale()\n",
    "    block_0_weight_scale3 = quant_bottleneck_model.quant_block0_conv3.quant_weight_scale()\n",
    "    block_0_weight_scale_skip = quant_bottleneck_model.shortcut.quant_weight_scale()\n",
    "    \n",
    "\n",
    "    # Block 2\n",
    "    block_1_relu_1 = quant_bottleneck_model.quant_block1_relu1.quant_act_scale()\n",
    "    block_1_relu_2= quant_bottleneck_model.quant_block1_relu2.quant_act_scale()\n",
    "    block_1_relu_3= quant_bottleneck_model.quant_block1_relu3.quant_act_scale()\n",
    "    \n",
    "    block_1_weight_scale1 = quant_bottleneck_model.quant_block1_conv1.quant_weight_scale()\n",
    "    block_1_weight_scale2 = quant_bottleneck_model.quant_block1_conv2.quant_weight_scale()\n",
    "    block_1_weight_scale3 = quant_bottleneck_model.quant_block1_conv3.quant_weight_scale()\n",
    "    block_1_quant_add_1 = quant_bottleneck_model.quant_add_1.quant_act_scale()\n",
    "\n",
    "    block_0_int_weight_1 = quant_bottleneck_model.quant_block0_conv1.quant_weight().int(float_datatype=True)\n",
    "    block_0_int_weight_2 = quant_bottleneck_model.quant_block0_conv2.quant_weight().int(float_datatype=True)\n",
    "    block_0_int_weight_3 = quant_bottleneck_model.quant_block0_conv3.quant_weight().int(float_datatype=True)\n",
    "    block_0_int_weight_skip = quant_bottleneck_model.shortcut.quant_weight().int(float_datatype=True)\n",
    "\n",
    "    block_1_int_weight_1 = quant_bottleneck_model.quant_block1_conv1.quant_weight().int(float_datatype=True)\n",
    "    block_1_int_weight_2 = quant_bottleneck_model.quant_block1_conv2.quant_weight().int(float_datatype=True)\n",
    "    block_1_int_weight_3 = quant_bottleneck_model.quant_block1_conv3.quant_weight().int(float_datatype=True)\n",
    "    \n",
    "    block_0_combined_scale1=-torch.log2(init_scale*block_0_weight_scale1/block_0_relu_1) # RHS after first conv1x1 | clip 0-->255\n",
    "    block_0_combined_scale2=-torch.log2(block_0_relu_1*block_0_weight_scale2/block_0_relu_2) # RHS after second conv3x3 | clip 0-->255\n",
    "    block_0_combined_scale3=-torch.log2(block_0_relu_2*block_0_weight_scale3/init_scale) # RHS after third conv1x1 | clip -128-->+127\n",
    "    block_0_combined_scale_skip=-torch.log2(init_scale*block_0_weight_scale_skip/init_scale) # LHS after conv1x1 | clip -128-->+127\n",
    "    block_0_combined_scale4=-torch.log2(init_scale/block_0_relu_3) # After addition | clip 0-->255\n",
    "    \n",
    "    block_1_combined_scale1=-torch.log2(block_0_relu_3*block_1_weight_scale1/block_1_relu_1) # RHS after first conv1x1 | clip 0-->255\n",
    "    block_1_combined_scale2=-torch.log2(block_1_relu_1*block_1_weight_scale2/block_1_relu_2) # RHS after second conv3x3 | clip 0-->255\n",
    "    block_1_combined_scale3=-torch.log2(block_1_relu_2*block_1_weight_scale3/block_1_quant_add_1) # RHS after third conv1x1 | clip -128-->+127\n",
    "    block_1_combined_scale4=-torch.log2(block_1_quant_add_1/block_1_relu_3) # After addition | clip 0-->255\n",
    "    print(\"_________POST PTQ SCALES_________\")\n",
    "    print(\"init_scale:\",init_scale)\n",
    "    print(\"block_0_relu1:\",block_0_relu_1)\n",
    "    print(\"block_0_relu2:\",block_0_relu_2)\n",
    "    print(\"block_0_relu3:\",block_0_relu_3)\n",
    "    \n",
    "    print(\"block_0_weight_scale1:\",block_0_weight_scale1)\n",
    "    print(\"block_0_weight_scale2:\",block_0_weight_scale2)\n",
    "    print(\"block_0_weight_scale3:\",block_0_weight_scale3)\n",
    "    print(\"block_0_weight_scale_skip:\",block_0_weight_scale_skip)\n",
    "\n",
    "    print(\"init_scale:\",init_scale)\n",
    "    print(\"block_1_relu1:\",block_1_relu_1)\n",
    "    print(\"block_1_relu2:\",block_1_relu_2)\n",
    "    print(\"block_1_relu3:\",block_1_relu_3)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"block_1_weight_scale1:\",block_1_weight_scale1)\n",
    "    print(\"block_1_weight_scale2:\",block_1_weight_scale2)\n",
    "    print(\"block_1_weight_scale3:\",block_1_weight_scale3)\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"combined_scale block0 after first conv1x1:\",block_0_combined_scale1.item())\n",
    "    print(\"combined_scale block0 after second conv3x3:\",block_0_combined_scale2.item())\n",
    "    print(\"combined_scale block0 after third conv1x1:\",block_0_combined_scale3.item())\n",
    "    print(\"combined_scale block0 after adding skip connection:\",(block_0_combined_scale4).item())\n",
    "    print(\"combined_scale block0 after skip conv1x1:\",block_0_combined_scale_skip.item())\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"combined_scale block1 after first conv1x1:\",block_1_combined_scale1.item())\n",
    "    print(\"combined_scale block1 after second conv3x3:\",block_1_combined_scale2.item())\n",
    "    print(\"combined_scale block1 after third conv1x1:\",block_1_combined_scale3.item())\n",
    "    print(\"combined_scale block1 after adding skip connection:\",(block_1_combined_scale4).item())\n",
    "\n",
    "    q_bottleneck_out=quant_bottleneck_model(input)\n",
    "    gold_out=q_bottleneck_out.int(float_datatype=True).data.numpy().astype(dtype_out)\n",
    "    print(\"Golden::Brevitas::\",gold_out)\n",
    "    gold_out.tofile(log_folder+\"/gold_out.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    from brevitas.export import export_onnx_qcdq\n",
    "    # ref_input = torch.ones(1, 3, 32, 32, device=\"cpu\", dtype=dtype)\n",
    "    export_onnx_qcdq(quant_bottleneck_model, input, log_folder+\"/\"+design+\".onnx\")\n",
    "    # # Brevitas convolution\n",
    "    q_inp = quant_id_1(input)\n",
    "    int_inp = q_inp.int(float_datatype=True)\n",
    "    \n",
    "    before_input=int_inp.squeeze().data.numpy().astype(dtype_in)\n",
    "\n",
    "    before_input.tofile(log_folder+\"/before_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\")\n",
    "    ifm_mem_fmt = ds.reorder_mat(before_input,'YCXC8' , 'CYX' )\n",
    "    ifm_mem_fmt.tofile(log_folder+\"/after_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\")\n",
    "    block0_wts1 = ds.reorder_mat(block_0_int_weight_1.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "    block0_wts2 = ds.reorder_mat(block_0_int_weight_2.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "    block0_wts3 = ds.reorder_mat(block_0_int_weight_3.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "    block0_wts_skip = ds.reorder_mat(block_0_int_weight_skip.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "\n",
    "    total_wts=np.concatenate((block0_wts1,block0_wts2,block0_wts3,block0_wts_skip),axis=None)\n",
    "\n",
    "    block1_wts1 = ds.reorder_mat(block_1_int_weight_1.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "    block1_wts2 = ds.reorder_mat(block_1_int_weight_2.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "    block1_wts3 = ds.reorder_mat(block_1_int_weight_3.data.numpy().astype(dtype_wts),'OIYXI8O8' , 'OIYX' )\n",
    "\n",
    "    total_wts2=np.concatenate((total_wts,block1_wts1,block1_wts2,block1_wts3),axis=None)\n",
    "\n",
    "    total_wts2.tofile(log_folder+\"/weights_mem_fmt_final.txt\", sep=\",\", format=\"%d\")\n",
    "    print(\"total_wts\", total_wts2.shape)\n",
    "    for i in range (0,2):\n",
    "        app.buffers[2].write(ifm_mem_fmt)# input's standard format CYX | scalar YCX\n",
    "        app.buffers[3].write(total_wts2) # wts's standard format OIYX | scalar OIYX\n",
    "        # app.buffers[3].write(int_weight2.data.numpy().astype(dtype_in),offset=2048) # wts's standard format OIYX | scalar OIYX\n",
    "        app.run()\n",
    "        output3= app.buffers[4].read()\n",
    "        if enable_trace:\n",
    "            output3, trace = extract_trace(output3, shape_out, dtype_out)\n",
    "            write_out_trace(trace, trace_file)\n",
    "    # temp_out=output3.reshape(32,256, 32)\n",
    "    # ofm_mem_fmt = temp_out.swapaxes(0,1)\n",
    "    temp_out    = output3.reshape(32,32,32,8)\n",
    "    temp2_out   = ds.reorder_mat( temp_out, 'CDYX','YCXD' )\n",
    "    ofm_mem_fmt = temp2_out.reshape(256,32,32)   \n",
    "    ofm_mem_fmt.tofile(log_folder+\"/after_ofm_mem_fmt_final.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    ofm_mem_fmt=torch.from_numpy(ofm_mem_fmt).unsqueeze(0)\n",
    "    print(\"AIE output:::\",ofm_mem_fmt)\n",
    "    print(type(ofm_mem_fmt))\n",
    "    print(type(q_bottleneck_out))\n",
    "    print(\"difference::\",torch.max(torch.abs(ofm_mem_fmt - gold_out)))\n",
    "\n",
    "    print(\"difference::\",torch.max(torch.abs(ofm_mem_fmt*block_1_relu_3 - q_bottleneck_out)))\n",
    "    assert(np.allclose(ofm_mem_fmt, gold_out, rtol=0, atol=2.))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5c9e7e-997d-4e29-87e9-6dc4f69113a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if (enable_trace):\n",
    "    print(trace)\n",
    "else:\n",
    "    print (\"tracing not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31861e3-73d3-4197-b802-195c1573eedd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b36a4b-b6b2-4c32-9292-8a0994df8434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
