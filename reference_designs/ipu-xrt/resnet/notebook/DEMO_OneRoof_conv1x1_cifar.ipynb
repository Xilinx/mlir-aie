{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ac7c13-4c9d-4e2e-a2c4-ed84a69a6184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "try {\n",
       "require(['notebook/js/codecell'], function(codecell) {\n",
       "  codecell.CodeCell.options_default.highlight_modes[\n",
       "      'magic_text/x-csrc'] = {'reg':[/^%%microblaze/]};\n",
       "  Jupyter.notebook.events.one('kernel_ready.Kernel', function(){\n",
       "      Jupyter.notebook.get_cells().map(function(cell){\n",
       "          if (cell.cell_type == 'code'){ cell.auto_highlight(); } }) ;\n",
       "  });\n",
       "});\n",
       "} catch (e) {};\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale factor after first conv1x1: 9.0\n",
      "TEST PASS: AIE output matches golden quantized output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jacklo\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py:1269: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/core/TensorImpl.h:1791.)\n",
      "  return super().rename_(names)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../misc\")\n",
    "from utils import DataShaper\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import (\n",
    "    Int8ActPerTensorFixedPoint,\n",
    "    Int8WeightPerTensorFixedPoint,\n",
    "    Uint8ActPerTensorFixedPoint,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "design = \"conv1x1_cifar_vector\"\n",
    "# aie_teardown()\n",
    "sys.path.append(\"../../../utils\")\n",
    "import xrtutils\n",
    "\n",
    "xclbin_path = os.path.abspath(\"../baseline/\" + design + \"/build/final.xclbin\")\n",
    "insts_path = os.path.abspath(\"../baseline/\" + design + \"/build/insts.txt\")\n",
    "\n",
    "log_folder = \"log/log_\" + design\n",
    "\n",
    "enable_aie = True\n",
    "aie_is_setup = False\n",
    "enable_trace = True\n",
    "trace_file = \"traces/\" + design + \".txt\"\n",
    "\n",
    "app = None\n",
    "in_buf = None\n",
    "arg1_buf = None\n",
    "out_buf = None\n",
    "\n",
    "dtype_in = np.dtype(\"int8\")\n",
    "dtype_wts = np.dtype(\"int8\")\n",
    "dtype_out = np.dtype(\"uint8\")\n",
    "\n",
    "\n",
    "shape_in_act = (32, 8, 32, 8)  #'YCXC8' , 'CYX'\n",
    "shape_in_wts1 = (8, 8, 1, 1, 8, 8)  # out,in,ky,kx,in8,out8\n",
    "\n",
    "shape_total_wts = (4096, 1)\n",
    "shape_out = (32, 8, 32, 8)\n",
    "\n",
    "trace_size = 16384\n",
    "\n",
    "\n",
    "def setup_aie(\n",
    "    xclbin_path,\n",
    "    insts_path,\n",
    "    in_0_shape,\n",
    "    in_0_dtype,\n",
    "    in_1_shape,\n",
    "    in_1_dtype,\n",
    "    out_buf_shape,\n",
    "    out_buf_dtype,\n",
    "    enable_trace=False,\n",
    "    kernel_name=\"MLIR_AIE\",\n",
    "):\n",
    "    app = xrtutils.AIE_Application(xclbin_path, insts_path, kernel_name)\n",
    "    app.register_buffer(2, shape=in_0_shape, dtype=in_0_dtype)\n",
    "    app.register_buffer(3, shape=in_1_shape, dtype=in_1_dtype)\n",
    "    if enable_trace:\n",
    "        out_buf_len_bytes = np.prod(out_buf_shape) * np.dtype(out_buf_dtype).itemsize\n",
    "        out_buf_shape = (out_buf_len_bytes + trace_size,)\n",
    "        out_buf_dtype = np.uint8\n",
    "    app.register_buffer(4, shape=out_buf_shape, dtype=out_buf_dtype)\n",
    "    return app\n",
    "\n",
    "\n",
    "def extract_trace(out_buf, out_buf_shape, out_buf_dtype):\n",
    "    trace_size_words = trace_size // 4\n",
    "    out_buf_flat = out_buf.reshape((-1,)).view(np.uint32)\n",
    "    output_prefix = (\n",
    "        out_buf_flat[:-trace_size_words].view(out_buf_dtype).reshape(out_buf_shape)\n",
    "    )\n",
    "    trace_suffix = out_buf_flat[-trace_size_words:]\n",
    "    return output_prefix, trace_suffix\n",
    "\n",
    "\n",
    "def write_out_trace(trace, file_name):\n",
    "    out_str = \"\\n\".join(f\"{i:0{8}x}\" for i in trace if i != 0)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(out_str)\n",
    "\n",
    "\n",
    "app = setup_aie(\n",
    "    xclbin_path,\n",
    "    insts_path,\n",
    "    shape_in_act,\n",
    "    dtype_in,\n",
    "    shape_total_wts,\n",
    "    dtype_wts,\n",
    "    shape_out,\n",
    "    dtype_out,\n",
    "    enable_trace,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantReLU\n",
    "from brevitas.quant.fixed_point import (\n",
    "    Int8ActPerTensorFixedPoint,\n",
    "    Int8WeightPerTensorFixedPoint,\n",
    "    Uint8ActPerTensorFixedPoint,\n",
    ")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "if not os.path.exists(log_folder):\n",
    "    os.makedirs(log_folder)\n",
    "\n",
    "\n",
    "# input=torch.ones(64,32,32)\n",
    "input = torch.randn(1, 64, 32, 32)\n",
    "# image_name = f'./cifar_images/image_0.png'\n",
    "# img = Image.open(image_name)\n",
    "# input_tensor = cifar_test_transform(img)\n",
    "# input = input_tensor.unsqueeze(0)\n",
    "# Use a separate QuantIdentity so that the quantized output can be fed to both layers\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "ds = DataShaper()\n",
    "\n",
    "\n",
    "def init_pad_input(x, input_channels, desired_channels=4):\n",
    "    padding = torch.zeros(1, input_channels * (desired_channels - 1), 32, 32)\n",
    "    return torch.cat((x, padding), 1)\n",
    "\n",
    "\n",
    "# try:\n",
    "for i in range(0, 1):\n",
    "\n",
    "    class QuantBottleneck_projected(nn.Module):\n",
    "        expansion = 4\n",
    "\n",
    "        def __init__(self, in_planes=64, planes=64):\n",
    "            super(QuantBottleneck_projected, self).__init__()\n",
    "            self.quant_id_1 = QuantIdentity(\n",
    "                act_quant=Int8ActPerTensorFixedPoint,\n",
    "                bit_width=8,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "            self.quant_conv1 = QuantConv2d(\n",
    "                in_planes,\n",
    "                planes,\n",
    "                kernel_size=1,\n",
    "                bit_width=8,\n",
    "                weight_bit_width=8,\n",
    "                bias=False,\n",
    "                weight_quant=Int8WeightPerTensorFixedPoint,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "            self.quant_relu1 = QuantReLU(\n",
    "                act_quant=Uint8ActPerTensorFixedPoint,\n",
    "                bit_width=8,\n",
    "                return_quant_tensor=True,\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            out_q = self.quant_id_1(x)\n",
    "            out = self.quant_conv1(out_q)\n",
    "            out = self.quant_relu1(out)\n",
    "            return out\n",
    "\n",
    "    quant_bottleneck_model = QuantBottleneck_projected()\n",
    "\n",
    "    quant_id_1 = QuantIdentity(\n",
    "        act_quant=Int8ActPerTensorFixedPoint, bit_width=8, return_quant_tensor=True\n",
    "    )\n",
    "    quant_bottleneck_model.eval()\n",
    "    quant_id_1.eval()\n",
    "\n",
    "    init_scale = quant_bottleneck_model.quant_id_1.quant_act_scale()\n",
    "    block_0_relu_1 = quant_bottleneck_model.quant_relu1.quant_act_scale()\n",
    "\n",
    "    block_0_weight_scale1 = quant_bottleneck_model.quant_conv1.quant_weight_scale()\n",
    "\n",
    "    block_0_combined_scale1 = -torch.log2(\n",
    "        init_scale * block_0_weight_scale1 / block_0_relu_1\n",
    "    )\n",
    "    print(\"Scale factor after first conv1x1:\", block_0_combined_scale1.item())\n",
    "    block_0_int_weight_1 = quant_bottleneck_model.quant_conv1.quant_weight().int(\n",
    "        float_datatype=True\n",
    "    )\n",
    "\n",
    "    q_bottleneck_out = quant_bottleneck_model(input)\n",
    "    gold_out = q_bottleneck_out.int(float_datatype=True).data.numpy().astype(dtype_out)\n",
    "    gold_out.tofile(log_folder + \"/gold_out.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    # from brevitas.export import export_onnx_qcdq\n",
    "    # ref_input = torch.ones(1, 3, 32, 32, device=\"cpu\", dtype=dtype)\n",
    "    # export_onnx_qcdq(quant_bottleneck_model, input, log_folder+\"/\"+design+\".onnx\")\n",
    "    # # Brevitas convolution\n",
    "    q_inp = quant_id_1(input)\n",
    "    int_inp = q_inp.int(float_datatype=True)\n",
    "\n",
    "    before_input = int_inp.squeeze().data.numpy().astype(dtype_in)\n",
    "    before_input.tofile(\n",
    "        log_folder + \"/before_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\"\n",
    "    )\n",
    "\n",
    "    ifm_mem_fmt = ds.reorder_mat(before_input, \"YCXC8\", \"CYX\")\n",
    "    ifm_mem_fmt.tofile(log_folder + \"/after_ifm_mem_fmt_1x1.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    wts1 = ds.reorder_mat(\n",
    "        block_0_int_weight_1.data.numpy().astype(dtype_wts), \"OIYXI8O8\", \"OIYX\"\n",
    "    )\n",
    "\n",
    "    total_wts = np.concatenate((wts1), axis=None)\n",
    "    total_wts.tofile(log_folder + \"/weights_mem_fmt_final.txt\", sep=\",\", format=\"%d\")\n",
    "\n",
    "    for i in range(0, 1):\n",
    "        app.buffers[2].write(ifm_mem_fmt)  # input's standard format CYX | scalar YCX\n",
    "        app.buffers[3].write(total_wts)  # wts's standard format OIYX | scalar OIYX\n",
    "        app.run()\n",
    "        output3 = app.buffers[4].read()\n",
    "        if enable_trace:\n",
    "            output3, trace = extract_trace(output3, shape_out, dtype_out)\n",
    "            write_out_trace(trace, trace_file)\n",
    "\n",
    "    temp_out = output3.reshape(32, 8, 32, 8)\n",
    "\n",
    "    temp2_out = ds.reorder_mat(temp_out, \"CDYX\", \"YCXD\")\n",
    "    ofm_mem_fmt = temp2_out.reshape(64, 32, 32)\n",
    "    ofm_mem_fmt.tofile(\n",
    "        log_folder + \"/after_ofm_mem_fmt_final.txt\", sep=\",\", format=\"%d\"\n",
    "    )\n",
    "\n",
    "    ofm_mem_fmt = torch.from_numpy(ofm_mem_fmt).unsqueeze(0)\n",
    "    assert np.allclose(ofm_mem_fmt, gold_out, rtol=0, atol=2.0)\n",
    "    print(\"TEST PASS: AIE output matches golden quantized output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93c13fc6-5652-4ea8-bf19-b14cd5d1da5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2149711872 4026531840     320319 ... 3247489408 3247489408 3247497726]\n"
     ]
    }
   ],
   "source": [
    "if enable_trace:\n",
    "    print(trace)\n",
    "else:\n",
    "    print(\"tracing not enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f3a886-1c05-44b0-838f-a5707c7a2441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
